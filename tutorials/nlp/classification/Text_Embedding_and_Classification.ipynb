{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Text_Embedding_and_Classification.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":["5VnVKgmbYeVs"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"5VnVKgmbYeVs"},"source":["# WELCOME TO **\"TensorFlow for Neural Language Processing\" Series**  üòÅ \n","\n","TensorFlow makes it easy for beginners and experts to create machine learning models for desktop, mobile, web, and cloud. TensorFlow provides a collection of workflows to develop and train models using Python, JavaScript, or Swift, and to easily deploy in the cloud, on-prem, in the browser, or on-device no matter what language you use.\n","\n","We will see how we can gain insights into text data and hands-on on how to use those insights to train NLP models and perform some human mimicking tasks. Let‚Äôs dive in and look at some of the basics of NLP.\n","<br/> <br/>\n","**In this series of 4 project courses, you will learn practically how to build Natural Language Processing algorithms and learn how to create amazing models and build, train, and test Neural Networks in NLP with Tensorflow!** üòé"]},{"cell_type":"markdown","metadata":{"id":"QN2qE3DSYeVt"},"source":["\n","## üëâüèª Course 1: Text Embedding and Classification"]},{"cell_type":"markdown","metadata":{"id":"cpzNDT9kYeVu"},"source":["\n","\n","## üëâüèª Course 2: Semantic Similarity in Texts"]},{"cell_type":"markdown","metadata":{"id":"A2EMVL1HYeVv"},"source":["## üëâüèª Course 3: Sentiment Analysis in Texts"]},{"cell_type":"markdown","metadata":{"id":"NIGnYzhIYeVv"},"source":["## üëâüèª Project 4: Text Generation with RNNs"]},{"cell_type":"code","metadata":{"id":"iF6mvtGNYeVw"},"source":["print (\"Let's start with Course 1: Word and Text Embeddings\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ORy-KvWXGXBo"},"source":["# WELCOME to this guided project \"Text Embedding and Classification\"! üòÅ \n","#### This project course is part of \"Tensorflow for Natural Language Processing\" Series of project courses.<br/><br/>\n","\n","We will go through 5 tasks to implement our project:<br/><br/>\n","üëâüèª**Task 1:** Overview of the project and  Import the Libraries. <br/><br/>\n","üëâüèª**Task 2:** Analyzing the embeddings. <br/><br/>\n","üëâüèª**Task 3:** Use Embedding in Text Classification. <br/><br/>\n","üëâüèª**Task 4:** Create and Train the model. <br/><br/>\n","üëâüèª**Task 5:** Evaluate the model with Predictions. <br/><br/>\n","\n","At the end, you will practice an amazing exercise that's related to the project."]},{"cell_type":"markdown","metadata":{"id":"0vNiJef9YeV0"},"source":["\n","\n","## üëâüèª Task 1: Overview of the Project and Import the Libraries\n","\n","In this project, you will learn how to use text embeddings for text classification tasks, and you will train and evaluate a text classifier. <br/>\n","\n","The CORD-19 Swivel text embedding module from TF-Hub was built to support researchers analyzing natural languages text.<br/>\n","\n","In this project we will:\n","- Analyze semantically similar words in the embedding space\n","- Train a classifier on the SciCite dataset using the CORD-19 embeddings ‚ú®\n","\n","At the end of this project, you will try out an amazing Bonus Exercise! ü§©"]},{"cell_type":"code","metadata":{"id":"oVXAqYCzYeV1"},"source":["!pip install tfds-nightly \n","###install tfds-nightly and restart kernel before starting the project"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-10-02T11:26:34.793339Z","iopub.status.busy":"2020-10-02T11:26:34.792667Z","iopub.status.idle":"2020-10-02T11:26:41.967480Z","shell.execute_reply":"2020-10-02T11:26:41.966888Z"},"id":"Ym2nXOPuPV__"},"source":["import functools\n","import itertools\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import seaborn as sns\n","import pandas as pd\n","\n","import tensorflow as tf\n","\n","import tensorflow_datasets as tfds\n","import tensorflow_hub as hub\n","\n","from tqdm import trange"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_VgRRf2I7tER"},"source":["## üëâüèª Task 2: Analyzing the embeddings\n","\n","Let's start off by analyzing the embedding by calculating and plotting a correlation matrix between different terms. If the embedding learned to successfully capture the meaning of different words, the embedding vectors of semantically similar words should be close together. Let's take a look at some COVID-19 related terms.üëÄ"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-10-02T11:26:41.973413Z","iopub.status.busy":"2020-10-02T11:26:41.972756Z","iopub.status.idle":"2020-10-02T11:26:46.866986Z","shell.execute_reply":"2020-10-02T11:26:46.867430Z"},"id":"HNN_9bBKSLHU"},"source":["# Use the inner product between two embedding vectors as the similarity measure\n","def plot_correlation(labels, features):\n","  corr = np.inner(features, features)\n","  corr /= np.max(corr)\n","  sns.heatmap(corr, xticklabels=labels, yticklabels=labels)\n","\n","# Generate embeddings for some terms\n","queries = [\n","  # Related viruses\n","  'coronavirus', 'SARS', 'MERS',\n","  # Regions\n","  'Italy', 'Spain', 'Europe',\n","  # Symptoms\n","  'cough', 'fever', 'throat'\n","]\n","\n","module = hub.load('https://tfhub.dev/tensorflow/cord-19/swivel-128d/3')\n","embeddings = module(queries)\n","\n","plot_correlation( queries, embeddings )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bg-PGqtm8B7K"},"source":["We can see that the embedding successfully captured the meaning of the different terms. Each word is similar to the other words of its cluster (i.e. \"coronavirus\" highly correlates with \"SARS\" and \"MERS\"), while they are different from terms of other clusters (i.e. the similarity between \"SARS\" and \"Spain\" is close to 0).\n","\n","Now let's see how we can use these embeddings to solve a specific task."]},{"cell_type":"markdown","metadata":{"id":"DhN-_XkKYeWA"},"source":["## üëâüèª Task 3: Use Embedding in Text Classification"]},{"cell_type":"markdown","metadata":{"id":"idJ1jFmH7xMa"},"source":["### ‚≠êSciCite: Citation Intent Classification\n","\n","Now we will see how we can use the embedding for downstream tasks such as text classification. We'll use the [SciCite dataset](https://www.tensorflow.org/datasets/catalog/scicite) from TensorFlow Datasets to classify citation intents in academic papers. Given a sentence with a citation from an academic paper, classify whether the main intent of the citation is as background information, use of methods, or comparing results.üòä"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-10-02T11:26:46.878623Z","iopub.status.busy":"2020-10-02T11:26:46.876959Z","iopub.status.idle":"2020-10-02T11:26:47.083680Z","shell.execute_reply":"2020-10-02T11:26:47.084136Z"},"id":"Ghc-CzT8DDaZ"},"source":["builder = tfds.builder(name='scicite')\n","builder.download_and_prepare()\n","train_data, validation_data, test_data = builder.as_dataset(\n","    split=('train', 'validation', 'test'),\n","    as_supervised=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"both","execution":{"iopub.execute_input":"2020-10-02T11:26:47.091759Z","iopub.status.busy":"2020-10-02T11:26:47.090630Z","iopub.status.idle":"2020-10-02T11:26:47.154988Z","shell.execute_reply":"2020-10-02T11:26:47.155469Z"},"id":"CVjyBD0ZPh4Z"},"source":["# Let's take a look at a few labeled examples from the training set\n","NUM_EXAMPLES =  12 ### YOUR CODE HERE - type:\"integer\"\n","\n","TEXT_FEATURE_NAME = builder.info.supervised_keys[0]\n","LABEL_NAME = builder.info.supervised_keys[1]\n","\n","def label2str(numeric_label):\n","  m = builder.info.features[LABEL_NAME].names\n","  return m[numeric_label]\n","\n","data = next(iter(train_data.batch(NUM_EXAMPLES)))\n","\n","\n","pd.DataFrame({\n","    TEXT_FEATURE_NAME: [ex.numpy().decode('utf8') for ex in data[0]],\n","    LABEL_NAME: [label2str(x) for x in data[1]]\n","})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LvoE1DwGYeWI"},"source":["## üëâüèª Task 4: Create and Train the model"]},{"cell_type":"markdown","metadata":{"id":"MJZ-Mkf_YeWJ"},"source":["### ‚≠ê Creating the model"]},{"cell_type":"markdown","metadata":{"id":"65s9UpYJ_1ct"},"source":["\n","\n","We'll train a classifier on the [SciCite dataset](https://www.tensorflow.org/datasets/catalog/scicite) using Keras.  Let's build a model which use the CORD-19 embeddings with a classification layer on top.ü¶†"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-10-02T11:26:47.165764Z","iopub.status.busy":"2020-10-02T11:26:47.161189Z","iopub.status.idle":"2020-10-02T11:26:48.133191Z","shell.execute_reply":"2020-10-02T11:26:48.132673Z"},"id":"yZUclu8xBYlj"},"source":["# Hyperparameters { run: \"auto\" }\n","\n","EMBEDDING = 'https://tfhub.dev/tensorflow/cord-19/swivel-128d/3'  #@param {type: \"string\"}\n","TRAINABLE_MODULE = False  # type: \"boolean\"\n","\n","hub_layer = hub.KerasLayer(EMBEDDING, input_shape=[], \n","                           dtype=tf.string, trainable=TRAINABLE_MODULE)\n","\n","model = tf.keras.Sequential()\n","model.add(hub_layer)\n","\n","### YOURE CODE HERE - Create Dense Layers\n","model.add(tf.keras.layers.Dense(3))\n","\n","### YOURE CODE HERE - Print the model summary\n","model.summary()\n","\n","model.compile(optimizer='adam', ### YOURE CODE HERE - Choose optimizer\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy']) ### YOURE CODE HERE, Choose the Metrics"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"weZKWK-pLBll"},"source":["### ‚≠ê  Let's train and evaluate the model to see the performance on the SciCite task.üòÅ"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-10-02T11:26:48.138599Z","iopub.status.busy":"2020-10-02T11:26:48.137936Z","iopub.status.idle":"2020-10-02T11:27:39.655852Z","shell.execute_reply":"2020-10-02T11:27:39.656322Z"},"id":"cO1FWkZW2WS9"},"source":["EPOCHS =  40 ### YOURE CODE HERE - type: \"integer\"\n","BATCH_SIZE = 32 ### YOURE CODE HERE - type: \"integer\"\n","\n","history = model.fit(train_data.shuffle(10000).batch(BATCH_SIZE),\n","                    epochs=EPOCHS,\n","                    validation_data=validation_data.batch(BATCH_SIZE),\n","                    verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-10-02T11:27:39.664037Z","iopub.status.busy":"2020-10-02T11:27:39.662923Z","iopub.status.idle":"2020-10-02T11:27:39.665500Z","shell.execute_reply":"2020-10-02T11:27:39.664859Z"},"id":"2sKE7kEyLJQZ"},"source":["from matplotlib import pyplot as plt\n","def display_training_curves(training, validation, title, subplot):\n","  if subplot%10==1: # set up the subplots on the first call\n","    plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n","    plt.tight_layout()\n","  ax = plt.subplot(subplot)\n","  ax.set_facecolor('#F8F8F8')\n","  ax.plot(training)\n","  ax.plot(validation)\n","  ax.set_title('model '+ title)\n","  ax.set_ylabel(title)\n","  ax.set_xlabel('epoch')\n","  ax.legend(['train', 'valid.'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-10-02T11:27:39.686233Z","iopub.status.busy":"2020-10-02T11:27:39.683868Z","iopub.status.idle":"2020-10-02T11:27:40.045154Z","shell.execute_reply":"2020-10-02T11:27:40.045635Z"},"id":"nnQfxevhLKld"},"source":["display_training_curves(history.history['accuracy'], history.history['val_accuracy'], 'accuracy', 211)\n","display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 212)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BjvtOw72Lpyw"},"source":["## üëâüèª Task 5: Evaluate the model with Predictions\n","\n","And let's see how the model performs. Two values will be returned. Loss (a number which represents our error, lower values are better), and accuracy.üòâ"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-10-02T11:27:40.051620Z","iopub.status.busy":"2020-10-02T11:27:40.050525Z","iopub.status.idle":"2020-10-02T11:27:40.225937Z","shell.execute_reply":"2020-10-02T11:27:40.226363Z"},"id":"y0ExC8D0LX8m"},"source":["results = model.evaluate(test_data.batch(512), verbose=2)\n","\n","for name, value in zip(model.metrics_names, results):\n","  print('%s: %.3f' % (name, value))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dWp5OWeTL2EW"},"source":["We can see that the loss quickly decreases while especially the accuracy rapidly increases. Let's plot some examples to check how the prediction relates to the true labels:"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-10-02T11:27:40.233338Z","iopub.status.busy":"2020-10-02T11:27:40.232295Z","iopub.status.idle":"2020-10-02T11:27:40.451083Z","shell.execute_reply":"2020-10-02T11:27:40.450480Z"},"id":"VzHzAOaaOVC0"},"source":["prediction_dataset = next(iter(test_data.batch(20)))\n","\n","prediction_texts = [ex.numpy().decode('utf8') for ex in prediction_dataset[0]]\n","prediction_labels = [label2str(x) for x in prediction_dataset[1]]\n","\n","predictions = [label2str(x) for x in model.predict_classes(prediction_texts)]\n","\n","\n","pd.DataFrame({\n","    TEXT_FEATURE_NAME: prediction_texts,### YOURE CODE HERE\n","    LABEL_NAME: prediction_labels,### YOURE CODE HERE\n","    'prediction': predictions\n","})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OSGcrkE069_Q"},"source":["We can see that for this random sample, the model predicts the correct label most of the times, indicating that it can embed scientific sentences pretty well.üòé"]},{"cell_type":"markdown","metadata":{"id":"fSeHO6TOYeWh"},"source":["## Bonus: Extra Exercise!\n","##### Refresh Your Memory... üòã"]},{"cell_type":"markdown","metadata":{"id":"EszHxYmHYeWh"},"source":["In this exercise, we will practice word tokenization, sentence tokenization, and normalization in texts!"]},{"cell_type":"code","metadata":{"id":"J0EcZgpAYeWi"},"source":["!pip install nltk"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_FzlCFLCYeWk"},"source":["#import NLTK library\n","import nltk"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iomsFlWEYeWn"},"source":["# Sentence tokenizer breaks text paragraph into sentences.\n","from nltk.tokenize import sent_tokenize\n","nltk.download('punkt')\n","\n","text=\"\"\"Hello Mr. Smith, how are you doing today? The weather is great, and city is awesome.\n","The sky is pinkish-blue. You shouldn't eat cardboard\"\"\"\n","tokenized_text=sent_tokenize(text)\n","print(tokenized_text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bjzhQvb3YeWp"},"source":["# Word tokenizer breaks text paragraph into words.\n","from nltk.tokenize import word_tokenize\n","tokenized_word=word_tokenize(text)\n","print(tokenized_word)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VTy7icEWYeWs"},"source":["# Frequency Distribution\n","from nltk.probability import FreqDist\n","fdist = FreqDist(tokenized_word)\n","print(fdist)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6BIW2CzXYeWu"},"source":["fdist.most_common(2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WKK1YEp0YeWy"},"source":["# Frequency Distribution Plot\n","import matplotlib.pyplot as plt\n","fdist.plot(30,cumulative=False)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T1B_3RJYYeW1"},"source":["# In NLTK for removing stopwords, you need to create a list of stopwords\n","# and filter out your list of tokens from these words.\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","stop_words=set(stopwords.words(\"english\"))\n","print(stop_words)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SCmHOkvfYeW3"},"source":["# Removing stop words\n","nltk.download('punkt')\n","filtered_sent=[]\n","for w in tokenized_word:\n","    if w not in stop_words:\n","        filtered_sent.append(w)\n","print(\"Tokenized Sentence:\",tokenized_word)\n","print(\"Filterd Sentence:\",filtered_sent)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z7ZC0HLfYeW7"},"source":["# Normalization\n","\n","# Stemming is a process of linguistic normalization,\n","# which reduces words to their word root word or chops off the derivational affixes. \n","# For example, connection, connected, connecting word reduce to a common word \"connect\".\n","\n","# Stemming\n","from nltk.stem import PorterStemmer\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","\n","ps = PorterStemmer()\n","\n","stemmed_words=[]\n","for w in filtered_sent:\n","    stemmed_words.append(ps.stem(w))\n","\n","print(\"Filtered Sentence:\",filtered_sent)\n","print(\"Stemmed Sentence:\",stemmed_words)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PM9SQ7_iYeW_"},"source":["# Normalization\n","\n","# Lemmatization reduces words to their base word, which is linguistically correct lemmas. \n","# It transforms root word with the use of vocabulary and morphological analysis.\n","# Lemmatization is usually more sophisticated than stemming. \n","# Stemmer works on an individual word without knowledge of the context. \n","# For example, The word \"better\" has \"good\" as its lemma. \n","# This thing will miss by stemming because it requires a dictionary look-up.\n","\n","nltk.download('wordnet')\n","#performing stemming and Lemmatization\n","from nltk.stem.wordnet import WordNetLemmatizer\n","lem = WordNetLemmatizer()\n","\n","from nltk.stem.porter import PorterStemmer\n","stem = PorterStemmer()\n","\n","word = \"flying\"\n","print(\"Lemmatized Word:\",lem.lemmatize(word,\"v\"))\n","print(\"Stemmed Word:\",stem.stem(word))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"egBbka3oYeXD"},"source":["# POS Tagging\n","# The primary target of Part-of-Speech(POS) tagging is to identify the grammatical group of a given word. \n","# Whether it is a NOUN, PRONOUN, ADJECTIVE, VERB, ADVERBS, etc. based on the context. \n","# POS Tagging looks for relationships within the sentence and assigns a corresponding tag to the word.\n","nltk.download('averaged_perceptron_tagger')\n","\n","sent = \"Albert Einstein was born in Ulm, Germany in 1879.\"\n","\n","tokens=nltk.word_tokenize(sent)\n","print(tokens)\n","\n","nltk.pos_tag(tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rwkxM-PLYeXF"},"source":["Which of the following is true for neural networks?\n","1. The training time depends on the size of the network.\n","2. Neural networks can be simulated on a conventional computer.\n","3. Artificial neurons are identical in operation to biological ones.\n","4. All of the mentioned\n","5. (2) is true\n","6. (1) and (2) are true\n","7. None of the mentioned\n","<br/> . <br/> . <br/> . <br/> . <br/> . <br/> . <br/> . <br/> . <br/> . <br/> . <br/> . <br/> . <br/> . <br/> . <br/> . <br/> . <br/>  . <br/> . <br/> . <br/> . <br/> . <br/> . <br/> . <br/> . <br/> . <br/> . <br/> . <br/> . <br/> . <br/> . <br/> . <br/> . <br/> \n"]},{"cell_type":"markdown","metadata":{"id":"Gcnv9CUmYeXG"},"source":["Answer: 6\n","Explanation: The training time depends on the size of the network; the number of neuron is greater and therefore the number of possible ‚Äòstates‚Äô is increased. Neural networks can be simulated on a conventional computer but the main advantage of neural networks ‚Äì parallel execution ‚Äì is lost. Artificial neurons are not identical in operation to the biological ones."]},{"cell_type":"markdown","metadata":{"id":"UTkQNfKpYeXG"},"source":["# CONGRATULATIONS! ü§©"]},{"cell_type":"code","metadata":{"id":"fzHS6O0RYeXH"},"source":[],"execution_count":null,"outputs":[]}]}